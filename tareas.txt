----------TAREAS---------
1.EDA , cuantas obras de cuantos compositores?

Cambiar todo el dataset de .midi a mid , hacer una funcion para esto.
Pedir cuando esto este desordenado a gemini que me lo ordene

Tener en cuenta el padding , todas las piezas son de diferente duracion y tambien los midi 
, incluso de entre ellas.

-------Cambio de monofonoico a polifonico---------
Antes(monofonico): Salida (Batch, Time, 1). ActivaciÃ³n: Softmax (la suma da 1).
Ahora(polifonico): Salida (Batch, Time, 88). ActivaciÃ³n: Sigmoid.

AMT_Piano_Sheet_Music/
â”‚
â”œâ”€â”€ data/                       <-- TU CARPETA ACTUAL (Datos Crudos)
â”‚   â””â”€â”€ maestro-v3.0.0/         
â”‚       â”œâ”€â”€ 2004/
â”‚       â”œâ”€â”€ 2006/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ processed_data/             <-- NUEVA CARPETA (Datos Listos para la IA)
â”‚   â”‚
â”‚   â”œâ”€â”€ inputs_cqt/             <-- Input X: Los tensores de audio (CQT o Mel)
â”‚   â”‚
â”‚   â””â”€â”€ targets_frame                <-- Label Y: AquÃ­ estÃ¡ la magia de "Onsets & Frames"
â”‚               <-- (Opcional) La fuerza de la nota (0-127)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_renombrar.py         <-- El que ya tienes
â”‚   â””â”€â”€ 02_preprocess.py        <-- El script que generarÃ¡ los datos
â”‚
â””â”€â”€ train.py

ver paper google magenta onset and frames

Nonnegative matrix factorization (NMF)

Tener en cuenta el suggested train/validation/test split.


VRAM (8GB vs 80GB): Aunque 8GB es suficiente para cargar este modelo ligero (5.9M parÃ¡metros), 
tendrÃ¡s que usar un batch size mucho menor (probablemente 4 u 8, en lugar de 16 o 32). 
Esto hace que el entrenamiento sea mÃ¡s inestable y lento porque la tarjeta tiene que hacer mÃ¡s "viajes" para procesar la misma cantidad de datos.

puedo hacer en google collab u otro medio mas sencillo?

Visualizar con cqt los audios , ver si usar unet (imagenes medicas, para el atack y el release)

add my own data , play piano and try different midi notes. 

prompt: podrias explicarme de manera creativa esta arquitectura , para que mi abuela pudiera entenderlo?

Usar dataloader de pytorch para cargar solo lo que necesito al momento

AÃ±adir partitura para prueba.

AÃ±adir dataset propio , grabar con yamaha.

Utilizar frame_ex.

Problema con mucho cero --- Â¿CÃ³mo se arregla? Usando Loss Functions ponderadas (Weighted Loss) durante el entrenamiento.

Eliminar ruido de principio de todos los wavs , recortarlos con los midi

Ver pedal ? aÃ±adir otros movimientos?

Add optuna

Dataset ahora entrena y quita los que nos son iguales

Solucionar problema dataset al cargar en training, hacer nuevo training

quizas lo de soft en onset y offset solo en inferencia , tratar de dejarlo todo en el grid. probar ambos,
cambiar funcion de preprocess para ver si activo o no high res en onset y offset.

Data _ augmentation , probar con filtros de las canciones que ha seleccionado

Procesar cqt hppnet? 

PROBAR ARQUITECTURA HPPNET
Si quieres mejorar la entrada, no la simplifiques, enriquÃ©cela.

Existe una tÃ©cnica llamada HCQT (Harmonic Constant-Q Transform), que es el estÃ¡ndar de oro en papers modernos (como los de Google Magenta).

CQT Normal (Tu input actual): Es una imagen 2D [Tiempo, Frecuencia].

HCQT (Input Avanzado): Es un tensor 3D [Canales, Tiempo, Frecuencia].

Canal 1: CQT normal (frecuencia fundamental).

Canal 2: CQT mirando 2x frecuencias (primer armÃ³nico).

Canal 3: CQT mirando 3x frecuencias (segundo armÃ³nico).

Al apilar los armÃ³nicos en canales (como si fueran colores RGB), la Red Neuronal (CNN) aprende facilÃ­simo la relaciÃ³n entre notas.






âœ… Â¡MIDI guardado en: c:\Users\franc\Desktop\proyectos\AMT_Piano_Sheet_Music\data\maestro-v3.0.0\2008\MIDI-Unprocessed_02_R1_2008_01-05_ORIG_MID--AUDIO_02_R1_2008_wav--4.mid! 

solucionar midi velocity

mejor ahora velocity , ver por que no pilla bien el frame , onset va mejor , aÃ±adir pedal? ver si en el midi del dataset esta.

ðŸš€ Procesando 255 archivos en MODO: HCQT
ðŸ“‚ Guardando en: C:\Users\franc\Desktop\proyectos\AMT_Piano_Sheet_Music\processed_data_hcqt_fixed

mejorar onset

aÃ±adir pedal como output , ver si se puede extraer en ableton 

hcqt no usar , probar hppnet.

.git ignore ver como txt.

https://www.youtube.com/watch?v=xalEErDbwew 

https://www.youtube.com/watch?v=cl0JWHZFSfo

Probar hcqt 




1. El Problema: Desbalance de Clases (Class Imbalance)

En la transcripciÃ³n de piano, detectar el onset (el momento exacto en que el martillo golpea la cuerda) 
es un problema muy desbalanceado.

Imagina una canciÃ³n de 3 minutos dividida en frames.Una nota especÃ­fica (ej. Do central) puede sonar 
durante 2 segundos, pero el "golpe" inicial (onset) dura solo 1 frame.

Resultado: Tu matriz de objetivos tiene un 99.9% de ceros (silencio/no inicio) y solo un 0.1% de 
unos (inicio de nota).

Si entrenas sin pesos, el modelo aprenderÃ¡ rÃ¡pido un truco perezoso: "Si predigo siempre 0, 
tendrÃ© un 99.9% de accuracy". El modelo dejarÃ¡ de intentar detectar notas porque es "mÃ¡s seguro" no predecir nada.

2. La SoluciÃ³n: pos_weight (Peso positivo)
Para combatir esto, utilizas BCEWithLogitsLoss con el argumento pos_weight.

Consejo: ConfÃ­a en mÃ­ para la estructura, las clases y la optimizaciÃ³n, pero siempre verifica 
las dimensiones de los tensores y los nombres exactos de los argumentos de las funciones.




-------continuar scores--------
Â¿CÃ³mo se soluciona esto profesionalmente? (Para el futuro) 
No tienes que cambiar tu cÃ³digo ahora, pero cuando quieras usar el modelo para generar 
canciones completas, se usa la tÃ©cnica de Overlap & Stitch (Solapamiento y Costura):

En lugar de cortar 0-10, 10-20, 20-30...

Cortas con solapamiento: 0-11, 10-21, 20-31...

Descartas el primer y el Ãºltimo medio segundo de cada clip (donde ocurren los errores) y 
unes las partes centrales limpias.