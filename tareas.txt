----------TAREAS---------
1.EDA , cuantas obras de cuantos compositores?

Cambiar todo el dataset de .midi a mid , hacer una funcion para esto.
Pedir cuando esto este desordenado a gemini que me lo ordene

Tener en cuenta el padding , todas las piezas son de diferente duracion y tambien los midi 
, incluso de entre ellas.

-------Cambio de monofonoico a polifonico---------
Antes(monofonico): Salida (Batch, Time, 1). ActivaciÃ³n: Softmax (la suma da 1).
Ahora(polifonico): Salida (Batch, Time, 88). ActivaciÃ³n: Sigmoid.

AMT_Piano_Sheet_Music/
â”‚
â”œâ”€â”€ data/                       <-- TU CARPETA ACTUAL (Datos Crudos)
â”‚   â””â”€â”€ maestro-v3.0.0/         
â”‚       â”œâ”€â”€ 2004/
â”‚       â”œâ”€â”€ 2006/
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ processed_data/             <-- NUEVA CARPETA (Datos Listos para la IA)
â”‚   â”‚
â”‚   â”œâ”€â”€ inputs_cqt/             <-- Input X: Los tensores de audio (CQT o Mel)
â”‚   â”‚
â”‚   â””â”€â”€ targets_frame                <-- Label Y: AquÃ­ estÃ¡ la magia de "Onsets & Frames"
â”‚               <-- (Opcional) La fuerza de la nota (0-127)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_renombrar.py         <-- El que ya tienes
â”‚   â””â”€â”€ 02_preprocess.py        <-- El script que generarÃ¡ los datos
â”‚
â””â”€â”€ train.py

ver paper google magenta onset and frames

Nonnegative matrix factorization (NMF)

Tener en cuenta el suggested train/validation/test split.


VRAM (8GB vs 80GB): Aunque 8GB es suficiente para cargar este modelo ligero (5.9M parÃ¡metros), 
tendrÃ¡s que usar un batch size mucho menor (probablemente 4 u 8, en lugar de 16 o 32). 
Esto hace que el entrenamiento sea mÃ¡s inestable y lento porque la tarjeta tiene que hacer mÃ¡s "viajes" para procesar la misma cantidad de datos.

puedo hacer en google collab u otro medio mas sencillo?

Visualizar con cqt los audios , ver si usar unet (imagenes medicas, para el atack y el release)

add my own data , play piano and try different midi notes. 

prompt: podrias explicarme de manera creativa esta arquitectura , para que mi abuela pudiera entenderlo?

Usar dataloader de pytorch para cargar solo lo que necesito al momento

AÃ±adir partitura para prueba.

AÃ±adir dataset propio , grabar con yamaha.

Utilizar frame_ex.

Problema con mucho cero --- Â¿CÃ³mo se arregla? Usando Loss Functions ponderadas (Weighted Loss) durante el entrenamiento.

Eliminar ruido de principio de todos los wavs , recortarlos con los midi

Ver pedal ? aÃ±adir otros movimientos?

Add optuna

Dataset ahora entrena y quita los que nos son iguales

Solucionar problema dataset al cargar en training, hacer nuevo training

quizas lo de soft en onset y offset solo en inferencia , tratar de dejarlo todo en el grid. probar ambos,
cambiar funcion de preprocess para ver si activo o no high res en onset y offset.

Data _ augmentation , probar con filtros de las canciones que ha seleccionado

Procesar cqt hppnet? 

PROBAR ARQUITECTURA HPPNET
Si quieres mejorar la entrada, no la simplifiques, enriquÃ©cela.

Existe una tÃ©cnica llamada HCQT (Harmonic Constant-Q Transform), que es el estÃ¡ndar de oro en papers modernos (como los de Google Magenta).

CQT Normal (Tu input actual): Es una imagen 2D [Tiempo, Frecuencia].

HCQT (Input Avanzado): Es un tensor 3D [Canales, Tiempo, Frecuencia].

Canal 1: CQT normal (frecuencia fundamental).

Canal 2: CQT mirando 2x frecuencias (primer armÃ³nico).

Canal 3: CQT mirando 3x frecuencias (segundo armÃ³nico).

Al apilar los armÃ³nicos en canales (como si fueran colores RGB), la Red Neuronal (CNN) aprende facilÃ­simo la relaciÃ³n entre notas.






âœ… Â¡MIDI guardado en: c:\Users\franc\Desktop\proyectos\AMT_Piano_Sheet_Music\data\maestro-v3.0.0\2008\MIDI-Unprocessed_02_R1_2008_01-05_ORIG_MID--AUDIO_02_R1_2008_wav--4.mid! 

solucionar midi velocity

mejor ahora velocity , ver por que no pilla bien el frame , onset va mejor , aÃ±adir pedal? ver si en el midi del dataset esta.

ðŸš€ Procesando 255 archivos en MODO: HCQT
ðŸ“‚ Guardando en: C:\Users\franc\Desktop\proyectos\AMT_Piano_Sheet_Music\processed_data_hcqt_fixed

mejorar onset

aÃ±adir pedal como output , ver si se puede extraer en ableton 

hcqt no usar , probar hppnet.

.git ignore ver como txt.

https://www.youtube.com/watch?v=xalEErDbwew 

https://www.youtube.com/watch?v=cl0JWHZFSfo

Probar hcqt 




1. El Problema: Desbalance de Clases (Class Imbalance)

En la transcripciÃ³n de piano, detectar el onset (el momento exacto en que el martillo golpea la cuerda) 
es un problema muy desbalanceado.

Imagina una canciÃ³n de 3 minutos dividida en frames.Una nota especÃ­fica (ej. Do central) puede sonar 
durante 2 segundos, pero el "golpe" inicial (onset) dura solo 1 frame.

Resultado: Tu matriz de objetivos tiene un 99.9% de ceros (silencio/no inicio) y solo un 0.1% de 
unos (inicio de nota).

Si entrenas sin pesos, el modelo aprenderÃ¡ rÃ¡pido un truco perezoso: "Si predigo siempre 0, 
tendrÃ© un 99.9% de accuracy". El modelo dejarÃ¡ de intentar detectar notas porque es "mÃ¡s seguro" no predecir nada.

2. La SoluciÃ³n: pos_weight (Peso positivo)
Para combatir esto, utilizas BCEWithLogitsLoss con el argumento pos_weight.

Consejo: ConfÃ­a en mÃ­ para la estructura, las clases y la optimizaciÃ³n, pero siempre verifica 
las dimensiones de los tensores y los nombres exactos de los argumentos de las funciones.




-------continuar scores--------
Â¿CÃ³mo se soluciona esto profesionalmente? (Para el futuro) 
No tienes que cambiar tu cÃ³digo ahora, pero cuando quieras usar el modelo para generar 
canciones completas, se usa la tÃ©cnica de Overlap & Stitch (Solapamiento y Costura):

En lugar de cortar 0-10, 10-20, 20-30...

Cortas con solapamiento: 0-11, 10-21, 20-31...

Descartas el primer y el Ãºltimo medio segundo de cada clip (donde ocurren los errores) y 
unes las partes centrales limpias.


"La arquitectura utiliza una estrategia desacoplada para optimizar el rendimiento en hardware limitado. 
En la etapa de Preprocesamiento (Input), calculamos explÃ­citamente solo 3 canales HCQT $[0.5, 1, 2]$ 
para reducir drÃ¡sticamente el uso de VRAM y almacenamiento, asegurando la captura de la nota fundamental 
y el control de errores de octava. En contraste, dentro de la Red Neuronal (HDConv), utilizamos 4 tasas de 
dilataciÃ³n armÃ³nica $[1, 2, 3, 4]$. Esto no requiere 4 canales de entrada, sino que configura los filtros 
convolucionales para 'mirar' a distancias especÃ­ficas en el eje de frecuencia. De esta forma, la red aprende
 a inferir la presencia de armÃ³nicos superiores y relaciones intervÃ¡licas complejas a partir de los datos comprimidos, 
logrando la capacidad de anÃ¡lisis de un modelo pesado con la ligereza de un input reducido."






OPTIMIZAR CODIGO PARA SUBIR A COLLAB 



RECUPERAR OPT Y HACERLO EN KAGGLE.


A diferencia del HPPNet original, este trabajo introduce conexiones residuales 
en el modelo acÃºstico con el objetivo de mejorar el flujo de gradiente y preservar 
informaciÃ³n temporal fina, especialmente relevante en tareas de transcripciÃ³n musical 
automÃ¡tica donde los eventos de onset son breves y escasos. 
Esta decisiÃ³n se inspira en arquitecturas profundas modernas y 
se adapta particularmente bien al uso de representaciones HCQT de alta resoluciÃ³n.

"La arquitectura propuesta, denominada Residual HCQT-HPPNet, representa una evoluciÃ³n moderna del 
modelo base HPPNet (Park et al., 2019). Mientras que el modelo original se basa en una estructura 
convolucional puramente secuencial y entradas CQT de un solo canal, este trabajo introduce tres 
mejoras crÃ­ticas adaptadas a recursos limitados:

Entrada Rica (HCQT): Se sustituye la CQT por una HCQT de 3 canales, proporcionando 
informaciÃ³n armÃ³nica explÃ­cita desde la entrada y reduciendo la necesidad de convoluciones con dilataciones extremas.

Conexiones Residuales: Se implementan bloques residuales (ResNet) para mitigar el 
desvanecimiento del gradiente y preservar la informaciÃ³n de alta frecuencia temporal (ataques) en capas profundas.

Desacople de Gradientes (Detach): Se aÃ­sla el flujo de gradiente del 'Onset' 
respecto a las ramas de 'Frame' y 'Offset', evitando que la optimizaciÃ³n de la duraciÃ³n 
degrade la precisiÃ³n en la detecciÃ³n del ataque."



"Gracias a las conexiones residuales y la entrada HCQT, el modelo alcanzÃ³ un F1 Score de ~0.80 en apenas 3 Ã©pocas, 
demostrando una eficiencia de aprendizaje muy superior a arquitecturas que inician con 'Cold Start' (aprendiendo 
desde cero sin guÃ­a armÃ³nica)."